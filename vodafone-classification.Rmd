---
title: "vodafone-classification"
author: "ahmed-akram"
date: "May 23, 2016"
output: html_document
---

```{r}
library(aod)
library(e1071)
library(rpart)
library(randomForest)
library(neuralnet)
library(dplyr)
library(ipred)

library(ggplot2)
library(RWeka)
library(forecast)
library(gbm)
library(extraTrees)
set.seed(10)

model_train <- read.csv("data/augmented_train.csv")
model_test <- read.csv("data/augmented_test.csv")
```

```{r cross validation function}
cross_validate <- function(data, model, class, folds, ntree = NULL, print=FALSE, type=NULL, frm=NULL) {
    # folds
    k = folds
    
    # give each row an id from 1:k representing which fold it's in
    data$id <- sample(1:k, nrow(data), replace = TRUE)
    list <- 1:k
    
    data[[class]] <- factor(data[[class]])
    
    progress.bar <- create_progress_bar("text")
    progress.bar$init(k)
    
    accuracies = c()
    precisions = c()
    recalls = c()
    f_scores = c()
    
    for (i in 1:k){
        # get all rows with id != i to be in training set and those with id == i will be testing set
        trainingset <- data %>% subset(id %in% list[-i])
        testset <- data %>% subset(id %in% c(i))
        # building the formula
        if(is.null(frm)) {
        	frm <- paste(class, ".", sep=" ~ ")
        }
        print(frm)
        # building the fitting model
        if(is.null(ntree)) {
            fit <- frm %>% formula %>% model(data=data)
        }else if(!is.null(type)) {
            fit <- frm %>% formula %>% model(data=data, type=type)
        }else{
            fit <- frm %>% formula %>% model(data=data, ntree = ntree)
        }
        
        # get the index of the class in the list of feature names
        index <- which(data %>% names == class)
        
        # predict on the test set without the desired class column
        pred <- predict(fit, testset[,-index])
        
        confusion_matrix <- table(pred, testset[[class]])
        #rownames(confusion_matrix) <- c("Predicted No", "Predicted Yes")
        #colnames(confusion_matrix) <- c("Actual No", "Actual Yes")
        
        if(print) {
            print(confusion_matrix)
        }
        
        TN <- confusion_matrix[1,1]
        TP <- confusion_matrix[2,2]
        FP <- confusion_matrix[2,1]
        FN <- confusion_matrix[1,2]
        
        accuracy <- (TP + TN) / (testset %>% nrow)
        precision <- (TP) / (FP + TP)
        recall <- (TP) / (TP + FN)
        f_score <- 2 * (recall * precision) / (recall + precision)
        
        accuracies <- accuracies %>% append(accuracy)
        precisions <- precisions %>% append(precision)
        recalls <- recalls %>% append(recall)
        f_scores <- f_scores %>% append(f_score)
        
        progress.bar$step()
    }
    
    return (as.data.frame(list("Accuracy" = accuracies, 
                               "Precision" = precisions,
                               "Recall" = recalls,
                               "F_Score" = f_scores)))
}
```

```{r divide into train and test for local validation}
divide_data <- function(data) {
nr <- NROW(data)
ind <- sample(nr, 0.7 * nr, replace = FALSE)
train <<- data[ind,]
test <<- data[-ind,]
}
divide_data(model_train)
```

```{r calculate function}
calculate <- function(fit) {
	pred <- predict(fit, newdata = test[, -12])
	confusion_matrix <- table(pred, test$TARGET)
	print(confusion_matrix)
	
	TN <- confusion_matrix[1,1]
	TP <- confusion_matrix[2,2]
	FP <- confusion_matrix[2,1]
	FN <- confusion_matrix[1,2]
	# accuracy <- (TP + TN) / (test %>% nrow)
	accuracy <- (TP + TN) / (TN + TP + FP + FN)
	print(accuracy)
	print(paste("accuracy ones ", (TP / (TP + FN))))
	print(paste("accuracy zeros ", (TN / (TN + FP))))
	return(TP)
}
```

```{r naive bayes}
fit <- naiveBayes(TARGET ~ TOTAL_USAGE + AVG_PER_SESSION + WEIGHTED_AVG + DIFF_BETWEEN_6th_AND_MEAN + BIGGER_THAN + GENDER, data = train)
calculate(fit)
```

```{r bagging}
fit <-  bagging(as.factor(TARGET) ~ TOTAL_USAGE + AVG_PER_SESSION + SUM_EXCEEDING + WEIGHTED_AVG + RATE_PLAN + AGE, data = train, type='class')
calculate(fit)
```

```{r decision tree}
fit <- J48(as.factor(TARGET) ~ TOTAL_USAGE + AVG_PER_SESSION + SUM_EXCEEDING + WEIGHTED_AVG, data = train)
calculate(fit)
```

```{r random forest}
fit <- randomForest(as.factor(TARGET) ~ TOTAL_USAGE + AVG_PER_SESSION + SUM_EXCEEDING + WEIGHTED_AVG, data = train, ntree=100)
calculate(fit)
```

# Submission 10:
```{r submission 10}
fit <- naiveBayes(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN, data=model_train)
# pred <- predict(fit, newdata = model_test)
submit(10, fit, model_test)
```

# Submission 11:
```{r submission 11}
fit <- J48(as.factor(TARGET) ~ AVG_PER_SESSION + DIFF_BETWEEN_6th_AND_MEAN + BIGGER_THAN, data=model_train)
# pred <- predict(fit, newdata = model_test)
submit(11, fit, model_test)
```

# Submission 12:
```{r}
fit <- naiveBayes(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN, data=model_train)

submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data$PREDICTED_TARGET <- data %>% apply(1, function(row) {
		if(as.numeric(row[['DIFF_BETWEEN_6th_AND_MEAN']]) > 135) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
	data$PREDICTED_TARGET <- data %>% apply(1, function(row) {
		if(as.numeric(row[['MONTH5_USAGE']]) > as.numeric(row[['MEAN_USAGE']]) + 500) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
	
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}

submit(12, fit, model_test)
```

# Submission 13:
Let's try some forecasting methods to try and project the usage of the 6th month, then we can simply check if it will pass the average of the past 5 months or not, or we can use the forecasted data in enhancing the performance of the models. I already tried the weighted average forecasting and added the 'WEIGHTED_AVG' feature. Now let's try the (drifting method), where the change in the forecasted data is affected by the average change in previous data.
```{r}
model_train$DRIFT_FORECAST <- model_train %>% apply(1, function(x) {
	cc <- as.numeric(
		c(
			x[['MONTH1_USAGE']], 
			x[['MONTH2_USAGE']], 
			x[['MONTH3_USAGE']], 
			x[['MONTH4_USAGE']], 
			x[['MONTH5_USAGE']]
		))
	frc <- rwf(cc, h=5, drift=TRUE)
	if(mean(frc$mean[1:3]) > 0) {
		return(mean(frc$mean[1:3]))
	}else {
		return(cc[5])	
	}
})

model_test$DRIFT_FORECAST <- model_test %>% apply(1, function(x) {
	cc <- as.numeric(
		c(
			x[['MONTH1_USAGE']], 
			x[['MONTH2_USAGE']], 
			x[['MONTH3_USAGE']], 
			x[['MONTH4_USAGE']], 
			x[['MONTH5_USAGE']]
		))
	frc <- rwf(cc, h=5, drift=TRUE)
	if(mean(frc$mean[1:3]) > 0) {
		return(mean(frc$mean[1:3]))
	}else {
		return(cc[5])	
	}
})
```

```{r drifted vs mean, cache=TRUE, fig.width=11, fig.height=8}
model_train %>% 
	subset(MEAN_USAGE < 10000) %>% 
	subset(DRIFT_FORECAST < 30000) %>% 
	ggplot(aes(x=MEAN_USAGE, y=DRIFT_FORECAST)) + 
	geom_point(aes(color=factor(TARGET))) +
	scale_color_manual("Target\n",labels = c("0", "1"), values = c("#EFADA9", "#69B5C7")) +
	theme(axis.title.y = element_text(size=12,angle=0,hjust=0.5,vjust=1,lineheight=40)) +
	theme(axis.text.x = element_text(size=10,angle=45)) +
	theme(axis.title.x = element_text(size=12)) +
	theme(plot.title = element_text(size=12)) +
	scale_x_continuous(breaks = floor(seq(min(model_train$MEAN_USAGE), max(model_train$MEAN_USAGE), by = 500))) +
  	scale_y_continuous(breaks = floor(seq(min(model_train$DRIFT_FORECAST), max(model_train$DRIFT_FORECAST), by = 1000))) +
	labs(title = "Drift Forecasted 6th month vs. Mean of 5 months", x="5 Months Mean Usage", y="6th Month usage")
```

```{r diff between drift and avg vs. avg, cache=TRUE, fig.width=11, fig.height=8}
model_train %>% 
	subset(MEAN_USAGE < 10000) %>% 
	subset(DRIFT_FORECAST < 30000) %>% 
	mutate(new_diff = DRIFT_FORECAST - MEAN_USAGE) %>%
	subset(new_diff < 1000) %>%
	subset(new_diff > -1000) %>%
	ggplot(aes(x=MEAN_USAGE, y=new_diff)) + 
	geom_point(aes(color=factor(TARGET))) +
	scale_color_manual("Target\n",labels = c("0", "1"), values = c("#EFADA9", "#69B5C7")) +
	theme(axis.title.y = element_text(size=12,angle=0,hjust=0.5,vjust=1,lineheight=40)) +
	theme(axis.text.x = element_text(size=10,angle=45)) +
	theme(axis.title.x = element_text(size=12)) +
	theme(plot.title = element_text(size=12)) +
	scale_x_continuous(breaks = floor(seq(min(model_train$MEAN_USAGE), max(model_train$MEAN_USAGE), by = 500))) +
  	scale_y_continuous(breaks = floor(seq(from = -1000, to=1000, by = 100))) +
	labs(title = "Forecasted 6th month vs. Mean of 5 months (Figure 5)", x="5 Months Mean Usage", y="6th Month usage")
```

```{r density of diff between drift and avg, cache=TRUE, fig.width=12, fig.height=7}
model_train %>% 
	mutate(new_diff = DRIFT_FORECAST - MEAN_USAGE) %>%
	subset(new_diff < 1000) %>%
	subset(new_diff > -1000) %>%
	ggplot(aes(new_diff, fill=factor(TARGET))) +
	scale_fill_manual("Target\n",labels = c("0", "1"), values = c("#EFADA9", "#69B5C7")) +
    geom_density(alpha=I(0.7)) +
	theme(axis.title.y = element_text(size=12,angle=0,hjust=0.5,vjust=1,lineheight=40)) +
	theme(axis.title.x = element_text(size=12)) +
	theme(axis.text.x = element_text(size=10,angle=45)) + 
	theme(plot.title = element_text(size=13,angle=0)) + 
	labs(title = "Density for (6th_Month - Five_Months_Avg) (Figure 6)", x="Difference between forecasted 6th month and average", y="Density") + 
	scale_x_continuous(breaks = ceiling(seq(min(model_train$DIFF_BETWEEN_6th_AND_MEAN), max(model_train$DIFF_BETWEEN_6th_AND_MEAN), by = 25)))
```


Let's train a naive bayes algorithm adding the new feature, and try a submission
```{r}
divide_data()
fit <- bagging(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + DRIFT_FORECAST, data=train)
pred <- predict(fit, newdata = test[,-12])
table <- table(pred, test$TARGET)
table
test$PRED = pred
```

```{r}
test %>% subset(PRED == 0) %>% subset(TARGET ==0) %>% subset(DRIFT_FORECAST > MEAN_USAGE + 400) %>% nrow / (test %>% subset(TARGET == 0) %>% nrow) * 100
test %>% subset(PRED == 0) %>% subset(TARGET ==1) %>% subset(DRIFT_FORECAST > MEAN_USAGE + 400) %>% nrow / (test %>% subset(TARGET == 1) %>% nrow) * 100
```
Now by running the above two commands, we can see if we change those predicted as no while their drift forecasted 6th month exceeds the mean usage by 400 to yes, it would screw up ~10% of the no cases, while correcting ~20% of the yes cases. If the data was fairly sampled between cases of yes and no (which is not the case here, 60% is no, 40% is yes) then this tradeoff would be fine.

```{r}
fit <- naiveBayes(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN + DRIFT_FORECAST, data=model_train)
submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data$PREDICTED_TARGET <- data %>% apply(1, function(row) {
		if(as.numeric(row[['MONTH5_USAGE']]) > as.numeric(row[['MEAN_USAGE']]) + 500) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
	data$PREDICTED_TARGET[data$DRIFT_FORECAST > data$MEAN_USAGE + 400] <- 1
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
submit(13, fit, model_test)
```
It scored the same as my best score so far (0.68609)


# Submission 14:

Let's train a bagging algorithm adding the new feature, and try a submission
```{r}
# divide_data()
fit <- bagging(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + DRIFT_FORECAST + WEIGHTED_AVG, data=train)
# fit <- bagging(as.factor(TARGET) ~ MEAN_USAGE + DRIFT_FORECAST + WEIGHTED_AVG, data=train)
pred <- predict(fit, newdata = test[,-12])
confusion_matrix <- table(pred, test$TARGET)
confusion_matrix
TN <- confusion_matrix[1,1]
TP <- confusion_matrix[2,2]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
# accuracy <- (TP + TN) / (test %>% nrow)
accuracy <- (TP + TN) / (TN + TP + FP + FN)
accuracy
test$PRED = pred
```

```{r}
fit <- bagging(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + DRIFT_FORECAST + WEIGHTED_AVG, data=model_train)

submit <- function(number, fit, data) {
	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data$PREDICTED_TARGET[data$DRIFT_FORECAST > data$MEAN_USAGE + 400] <- 1
	data$PREDICTED_TARGET[data$MONTH5_USAGE > data$MEAN_USAGE + 500] <- 1
	data$PREDICTED_TARGET[data$DIFF_BETWEEN_6th_AND_MEAN > 105] <- 1
	
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
submit(14, fit, model_test)
```
it scored (0.68506)


# Submission 15:
Outliers are bad for some of the classification models, while others are resistant to such outliers. Tree based models are not affected by outliers that much, because they split the data based on equations like “if predictor A is greater
than X, predict the class to be Y”. However, let's try to remove some of the extremes and see if that will affect our naiveBayes model.
```{r}
model_train <- model_train %>% subset(AVG_PER_SESSION < 10)
model_train <- model_train %>% subset(TOTAL_SESSIONS < 5000)
model_train <- model_train %>% subset(TOTAL_USAGE < 7000)
model_train <- model_train %>% subset(WEIGHTED_AVG < 1500)

fit <- naiveBayes(factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN + AVG_PER_SESSION, model_train)
```


```{r}
submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data$PREDICTED_TARGET[data$DRIFT_FORECAST > data$MEAN_USAGE + 400] <- 1
	data$PREDICTED_TARGET[data$SUM_EXCEEDING < 1] <- 0
data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
submit(15, fit, model_test)
```
It did improve the performance and we got a score of (0.70661) :)

# Submission 16:

```{r}
submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data$PREDICTED_TARGET <- data %>% apply(1, function(row) {
		if(as.numeric(row[['MONTH5_USAGE']]) > as.numeric(row[['MEAN_USAGE']]) + 500) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
	data$PREDICTED_TARGET[data$DRIFT_FORECAST > data$MEAN_USAGE + 400] <- 1
	data$PREDICTED_TARGET[data$SUM_EXCEEDING < 1] <- 0
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
submit(16, fit, model_test)
```

For this one all I did was adding the effect of the 'naive forecasting' method where the forecasted usage of the new month (6th one) is simply the usage of the last month. The score we got was the same as last submission.



# Submission 17:
```{r}
confusion_matrix = table(test$PRED, test$TARGET)
FN <- confusion_matrix[1,2]
FP <- confusion_matrix[2,1]
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
accuracy <- (TP + TN) / (TN + TP + FP + FN)
confusion_matrix
accuracy
```

```{r}
submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
submit(17, fit, model_test)
```

# Submission 18:
```{r}
fit <- naiveBayes(factor(TARGET) ~ TOTAL_USAGE + AVG_PER_SESSION + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN + DRIFT_FORECAST + diff_two2, data = model_train)
```

```{r}
submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	# data$PREDICTED_TARGET[data$DRIFT_FORECAST > data$MEAN_USAGE + 400] <- 1
	# data$PREDICTED_TARGET[data$SUM_EXCEEDING < 1] <- 0
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
submit(18, fit, model_test)
```

# Submission 19:
```{r}
pred <- predict(fit, newdata = temp[,-12])
	
temp$PRED = pred
temp$PRED <- temp %>% apply(1, function(row) {
		if(as.numeric(row[['MONTH5_USAGE']]) > as.numeric(row[['MEAN_USAGE']]) + 500) {
			return(1)
		}else {
			return(row[['PRED']])
		}
	})
# temp$PRED[temp$DRIFT_FORECAST > temp$MEAN_USAGE + 400] <- 1
temp$PRED[temp$SUM_EXCEEDING < 1] <- 0
```

```{r}
temp %>%
subset(DIFF_BETWEEN_6th_AND_MEAN > -500) %>%
subset(DIFF_BETWEEN_6th_AND_MEAN < 500) %>%
ggplot(aes(DIFF_BETWEEN_6th_AND_MEAN, fill=factor(PRED))) +
scale_fill_manual("Target\n",labels = c("0", "1"), values = c("#EFADA9", "#69B5C7")) +
geom_density(alpha=I(0.7))
```

```{r}
fit <- naiveBayes(factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN + AVG_PER_SESSION, model_train)

temp <- model_train
temp$TOTAL_USAGE <- scale(temp$TOTAL_USAGE)
temp$MEAN_USAGE <- scale(temp$MEAN_USAGE)
temp$WEIGHTED_AVG <- scale(temp$WEIGHTED_AVG)
temp$TOTAL_USAGE <- center(temp$TOTAL_USAGE)
temp$MEAN_USAGE <- center(temp$MEAN_USAGE)
temp$WEIGHTED_AVG <- center(temp$WEIGHTED_AVG)
```




# Submission 24:
Just a voting ensemble of all the past submissions, it got a score of 0.70543 which is not the best, but very good considering I took some very poor early submissions into consideration. Maybe I should try it again later, but only considering the hight score submissions.


# Random:
```{r}
fit <- glm(factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN + AVG_PER_SESSION, data=train, family = binomial())
			pred <- predict(fit, newdata = test, type = "response")
			test$PREDICTED_TARGET = pred
			test <- test %>% mutate(PREDICTED_TARGET = as.numeric(PREDICTED_TARGET >= 0.5))
```


```{r}
model_train$TOTAL_USAGE = tt$TOTAL_USAGE
model_train$MEAN_USAGE = tt$MEAN_USAGE
model_train$SUM_EXCEEDING = tt$SUM_EXCEEDING
model_train$WEIGHTED_AVG = tt$WEIGHTED_AVG
model_train$BIGGER_THAN = tt$BIGGER_THAN
model_train$AVG_PER_SESSION = tt$AVG_PER_SESSION

```

# Submisison xx:

Naivebayes is my best algorithm till now with an accuracy of (0.7818093) and correctly identifying 2183/4471 of the yes cases, but it missclassifies a 2649/18156 of the no cases which is a lot compared to other algorithms. So in this submission I'll try to build me a brigade of classifiers with the naivebayes as the main man (maybe giving it a higher weight) and the rest are supposed to bring down those no cases misses.

```{r}
# fit <- naiveBayes(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN, data=train)
# pred <- predict(fit, test[,c(-12, -34)])
# confusion_matrix <- table(pred, test$TARGET)
# confusion_matrix
# TN <- confusion_matrix[1,1]
# 	TP <- confusion_matrix[2,2]
# 	FP <- confusion_matrix[2,1]
# 	FN <- confusion_matrix[1,2]
# accuracy <- (TP + TN) / test %>% nrow
# accuracy

brigade <- function() {
	nb <- naiveBayes(as.factor(TARGET) ~ TOTAL_USAGE + SUM_EXCEEDING + MEAN_USAGE + WEIGHTED_AVG + BIGGER_THAN + MEAN_AVG_SESSION + MEAN_TOTAL_SESSION, data=train)
	
	dt <- J48(factor(TARGET) ~ AVG_PER_SESSION + DIFF_BETWEEN_6th_AND_MEAN + BIGGER_THAN, data = train)
	
	rf <- randomForest(factor(TARGET) ~ TOTAL_USAGE + MEAN_USAGE + WEIGHTED_AVG + DIFF_BETWEEN_6th_AND_MEAN, data = train, ntree=50)
	
	print("here")
	pred_nb <- predict(nb, test[,c(-12, -34)])
	pred_dt <- predict(dt, test[,c(-12, -34)])
	pred_rf <- predict(rf, test[,c(-12, -34)])
	
	
	pred <- (as.numeric(pred_nb) - 1) + (as.numeric(pred_dt) - 1) + (as.numeric(pred_rf) - 1)
	pred <- (pred / 3) %>% round(0)
	
	confusion_matrix <- table(pred, test$TARGET)
	print(confusion_matrix)
	TN <- confusion_matrix[1,1]
	TP <- confusion_matrix[2,2]
	FP <- confusion_matrix[2,1]
	FN <- confusion_matrix[1,2]
	accuracy <- (TP + TN) / test %>% nrow
	print(accuracy)
}
```

```{r}
model_train <- model_train %>% 
	mutate(temp1 = numeric(MONTH2_USAGE > MONTH1_USAGE + 500))

# model_train$MONTH1_USAGE <- as.numeric(model_train$MONTH1_USAGE)
# model_train$MONTH2_USAGE <- as.numeric(model_train$MONTH2_USAGE)
# model_train$MONTH3_USAGE <- as.numeric(model_train$MONTH3_USAGE)
# model_train$MONTH4_USAGE <- as.numeric(model_train$MONTH4_USAGE)
# model_train$MONTH5_USAGE <- as.numeric(model_train$MONTH5_USAGE)

model_train$temp0 <- model_train %>% apply(1, function(x) {
	if(x[['MONTH2_USAGE']] > mean(as.numeric(c(x[['MONTH1_USAGE']]))) + 500) {
		return(1)
	}
	return(0)
})

model_train$temp1 <- model_train %>% apply(1, function(x) {
	if(x[['MONTH3_USAGE']] > mean(as.numeric(c(x[['MONTH1_USAGE']], x[['MONTH2_USAGE']]))) + 500) {
		return(1)
	}
	return(0)
})

model_train$temp2 <- model_train %>% apply(1, function(x) {
	if(x[['MONTH4_USAGE']] > mean(as.numeric(c(x[['MONTH1_USAGE']], x[['MONTH2_USAGE']], x[['MONTH3_USAGE']]))) + 500) {
		return(1)
	}
	return(0)
})
model_train$temp3 <- model_train %>% apply(1, function(x) {
	if(x[['MONTH5_USAGE']] > mean(as.numeric(c(x[['MONTH1_USAGE']], x[['MONTH2_USAGE']], x[['MONTH3_USAGE']], x[['MONTH4_USAGE']]))) + 500) {
		return(1)
	}
	return(0)
})


model_train$CORRECT_SUM_EXCEED <- model_train$temp0 + model_train$temp1 + model_train$temp2 + model_train$temp3

# model_train$MEAN_AVG_SESSION[model_train$TARGET == 1] <- 10.917
# model_train$MEAN_AVG_SESSION[model_train$TARGET == 0] <- 5.8189
# 
# model_train$MEAN_TOTAL_SESSION[model_train$TARGET == 1] <- 4062
# model_train$MEAN_TOTAL_SESSION[model_train$TARGET == 0] <- 2836

# model_train$MONTH1_USAGE = mt$MONTH1_USAGE
# model_train$MONTH2_USAGE = mt$MONTH2_USAGE
# model_train$MONTH3_USAGE = mt$MONTH3_USAGE
# model_train$MONTH4_USAGE = mt$MONTH4_USAGE
# model_train$MONTH5_USAGE = mt$MONTH5_USAGE


# temp2 = numeric(MONTH3_USAGE > mean(c(MONTH1_USAGE, MONTH2_USAGE))),
# 		   temp3 = numeric(MONTH4_USAGE > mean(c(MONTH1_USAGE, MONTH2_USAGE, MONTH3_USAGE))),
# 		   temp4 = numeric(MONTH5_USAGE > mean(c(MONTH1_USAGE, MONTH2_USAGE, MONTH3_USAGE, MONTH4_USAGE)))
```




```{r ignore this for now}
calender_ref <- read.csv("data/calendar_ref.csv")
model_test <- merge(x = model_test, y = contract_ref, by = 'CONTRACT_KEY')
model_train <- merge(x = model_train, y = contract_ref, by = 'CONTRACT_KEY')


yes <- model_train %>% subset(TARGET == 1)
no <- model_train %>% subset(TARGET == 0)
svm_dataset <- yes %>% sample_n(size = 4000)
svm_dataset <- yes %>% sample_n(size = 4000)
svm_dataset %>% nrow
svm_dataset <- svm_dataset %>% rbind(no %>% sample_n(size= 16000))

svm_dataset <- svm_dataset[sample(nrow(svm_dataset)), ]
svm_dataset <- svm_dataset %>% dplyr::select(-diff_one, -diff_one2, -diff_two, -diff_two2, -diff_three, -diff_three2)

svm_dataset.with_nas <- svm_dataset
svm_dataset <- svm_dataset %>% dplyr::select(-GENDER, -ROAMING_COUNTER)

```

# Submission xx:
```{r submission 12 - general linera regression}
logit <- glm(TARGET ~ TOTAL_USAGE + TOTAL_SESSIONS + AVG_PER_SESSION + SUM_EXCEEDING + MEAN_USAGE, data = train)
logit %>% summary
```
As we can see, the model doesn't use the MEAN_USAGE feature, so let's drop it.
```{r}
logit <- glm(TARGET ~ TOTAL_USAGE + TOTAL_SESSIONS + AVG_PER_SESSION + SUM_EXCEEDING + DIFF_BETWEEN_6th_AND_MEAN, data=train, family=binomial)
logit %>% summary

exp(coef(logit))
```
From the exponentiated coefficients above, we can say that increasing one unit in most of the features used in the model, will increase thethe odds of the TARGET being 1 by ~ 1

```{r}
logit <- lm(TARGET ~ TOTAL_USAGE + TOTAL_SESSIONS + AVG_PER_SESSION + SUM_EXCEEDING + DIFF_BETWEEN_6th_AND_MEAN, data=train)
plot(logit)
```

```{r}
ee <- model_train %>% head(5)
ee %>% apply(1, function(x) {
	cc <- as.numeric(c(x[['MONTH1_USAGE']], x[['MONTH2_USAGE']], x[['MONTH3_USAGE']], x[['MONTH4_USAGE']], x[['MONTH5_USAGE']]))
	print(mean(cc))
	print(cc)
	f <- auto.arima(cc, d=0, xreg=c(1,2,3,4,5))
	print(f)
	print(x[['TARGET']])
	print("")
})
```


```{r try all funcion}
try_all <- function(model) {
# 	features <- c("TOTAL_USAGE", "BIGGER_THAN")
	features <- c("TOTAL_USAGE", "AVG_PER_SESSION", "SUM_EXCEEDING", "MEAN_USAGE", "WEIGHTED_AVG", "DIFF_BETWEEN_6th_AND_MEAN", "BIGGER_THAN")

	substrRight <- function(x, n){
		substr(x, nchar(x)-n+1, nchar(x))
	}
	
	max_score = 0
	best_form = ""
	
	train[["TARGET"]] <- factor(train[["TARGET"]])
	
	rec <- function(index, frm) {
		if(index > length(features)) {
			if(frm == "") {
				return(" .")
			}
			return(frm)
		}
		
		if(frm == "") {
			formula = paste(frm, features[index], sep=" ")
		}else {
			formula = paste(frm, features[index], sep=" + ")
		}
		
		used_formula <- formula
		frm2 <- paste("TARGET", used_formula, sep=" ~")
		print(frm2)
		
		if(model == 1) {
			fit <-  naiveBayes(as.formula(frm2), data = train)	
		}else if(model == 2) {
			fit <-  randomForest(as.formula(frm2), data = train, ntree=50)
		}else if(model == 3) {
			fit <-  bagging(as.formula(frm2), data = train)
		}else if(model == 4) {
			fit <-  J48(as.formula(frm2), data = train)
		}else if(model ==5) {
			fit <-  svm(as.formula(frm2), data = train, type="C-classification")			
		}else {
			fit <- glm(as.formula(frm2), data=train, family = binomial())
			pred <- predict(fit, newdata = test, type = "response")
			test$PREDICTED_TARGET = pred
			test <- test %>% mutate(PREDICTED_TARGET = as.numeric(PREDICTED_TARGET >= 0.5))
			confusion_matrix<- table(test$PREDICTED_TARGET, test$TARGET)
			print(confusion_matrix)
			TN <- confusion_matrix[1,1]
			TP <- confusion_matrix[2,2]
			FP <- confusion_matrix[2,1]
			FN <- confusion_matrix[1,2]
			# accuracy <- (TP + TN) / (test %>% nrow)
			accuracy <- (TP + TN) / (TN + TP + FP + FN)
			print(accuracy)
			res <- TP
		}
		if(model != 6) {
			res <- calculate(fit)	
		}
		
		if(as.numeric(res) > max_score) {
			best_form <<- frm2
			max_score <<- res
		}
		print("====================")
		rec(index + 1, formula)
		rec(index + 1, frm)
	}

	
	rec(1, "")
	print(paste("Best score achieved ", max_score, "with formula: ", best_form))
}
```

```{r try every possible formula attributes with the following algorithms}
sink("output/naivebayes.out", append=FALSE)
try_all(1)
sink()
sink()
print("Done [naiveBayes]")

sink("output/randomForest.out", append=FALSE)
try_all(2)
sink()
print("Done [randomForest]")

sink("output/bagging.out", append=FALSE)
try_all(3)
sink()
print("Done [bagging]")
# 
sink("output/J48.out", append=FALSE)
try_all(4)
sink()
print("Done [J48]")

sink("output/svm.out", append=FALSE)
try_all(5)
sink()
print("Done [svm]")

```




```{r}
# divide_data()
pred <- predict(fit, newdata = test)
test$PREDICTED_TARGET = pred
test$PREDICTED_TARGET <- test %>% apply(1, function(row) {
		if(as.numeric(row[['DIFF_BETWEEN_6th_AND_MEAN']]) > 135) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
confusion_matrix<- table(test$PREDICTED_TARGET, test$TARGET)
confusion_matrix
TN <- confusion_matrix[1,1]
TP <- confusion_matrix[2,2]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
accuracy <- (TP + TN) / (TN + TP + FP + FN)
accuracy
accuracy_one <-  TP / (FN + TP)
accuracy_zero <- TN / (TN + FP)
paste("accurace_one ", accuracy_one)
paste("accurace_zero ", accuracy_zero)
```



```{r submit}
submit <- function(number, fit, data) {

	pred <- predict(fit, newdata = data)
	print(pred %>% unique)
	
	data$PREDICTED_TARGET = pred
	data$PREDICTED_TARGET <- data %>% apply(1, function(row) {
		if(as.numeric(row[['DIFF_BETWEEN_6th_AND_MEAN']]) > 135) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
	data$PREDICTED_TARGET <- data %>% apply(1, function(row) {
		if(as.numeric(row[['MONTH5_USAGE']]) > as.numeric(row[['MEAN_USAGE']]) + 500) {
			return(1)
		}else {
			return(row[['PREDICTED_TARGET']])
		}
	})
	
	data <- data %>% dplyr::select(CONTRACT_KEY, PREDICTED_TARGET)
	
	file <- paste("submissions/submission-", number, ".csv", sep="")
	write.csv(data, file, row.names = FALSE)
}
```
